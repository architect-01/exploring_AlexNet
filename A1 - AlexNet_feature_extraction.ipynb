{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"A1 - AlexNet_feature_extraction.ipynb","provenance":[{"file_id":"1I7MDL5LpqBZST-W-umNLNKnJUI5wt3Cv","timestamp":1620294954558}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPoSgweLKB6th26GH14RZxw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"FCi3-TIuKWgY"},"source":["AlexNet trained using Transfer Learning (Feature Extraction)\n","------------\n","Training AlexNet model on Oxford-IIIT dataset using a variant of Transfer Learning.\n","\n","Parameters of all layers (except for the last fully connected layer) are frozen.\n","\n","Notes:\n","- Notebook expects that you have downloaded the dataset into your drive.\n","- Change the path to reflect your location."]},{"cell_type":"code","metadata":{"id":"7WViMhCr5AE_"},"source":["from google.colab import drive\n","\n","#mount the drive\n","MOUNTING_LOCATION = '/content/drive'\n","print('Mounting the drive...')\n","drive.mount(MOUNTING_LOCATION)\n","print(f\"Drive mounted at: {MOUNTING_LOCATION}\")\n","\n","#Unzip the train and validation datasets\n","print('Unzipping the train dataset...')\n","!unzip -q \"/content/drive/My Drive/DIPLOMSKI/oxford-iiit/train.zip\" -d '/content' \n","print('Finished unzipping the train dataset. Now unzipping the validation dataset...')\n","!unzip -q \"/content/drive/My Drive/DIPLOMSKI/oxford-iiit/val.zip\" -d '/content'\n","print('Finished unzipping the validation dataset.')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S9xHpyb98Ixw"},"source":["import torch\n","from PIL import Image\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import time\n","import os\n","import json\n","import copy\n","\n","dataset_props = {'DATA_DIR': '/content/',\n","                 'N_CLASSES': 37\n","                 }\n","\n","MODEL_NAME = 'A1'\n","model_props = {'MODEL_NAME': MODEL_NAME,\n","               'PRETRAINED': True,\n","               'SAVE_PATH': f'/content/drive/My Drive/DIPLOMSKI/{MODEL_NAME}/',\n","               'SAVE_EVERY_N_EPOCHS': 5,\n","               'TRAIN_NAME_SAVE': f'{MODEL_NAME}_train_model.pt',\n","               'VAL_NAME_SAVE': f'{MODEL_NAME}_val_model.pt',\n","               'LOG_NAME': f'{MODEL_NAME}_log.json',\n","               'INPUT_SIZE': 224,\n","               'N_EPOCHS': 1000,\n","               'BATCH_SIZE': 64,\n","               'LEARNING_RATE': 1e-4,\n","               'MOMENTUM': 0.9,\n","               'DEVICE': torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")}\n","\n","#transforms to be applied to images before them being fed to the model\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(model_props['INPUT_SIZE']),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(model_props['INPUT_SIZE']),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}\n","#building the dataset loaders\n","image_datasets = {x: datasets.ImageFolder(os.path.join(dataset_props['DATA_DIR'], x), \n","                                          data_transforms[x]) for x in ['train', 'val']}\n","dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x],\n","                                                   batch_size=model_props['BATCH_SIZE'],\n","                                                   shuffle=True) for x in ['train', 'val']}\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ePBpQcUt_e7v"},"source":["\n","#Build the Model\n","model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=model_props['PRETRAINED'])\n","\n","#freeze all parameters\n","for param in model.parameters():\n","  param.requires_grad = False\n","\n","#Replace the last fullyconnected layer - to match the number of classes in this dataset\n","num_ftrs = model.classifier[6].in_features\n","model.classifier[6] = nn.Linear(num_ftrs, dataset_props['N_CLASSES'])\n","\n","#put the model inside GPU memory if GPU is available\n","if model_props['DEVICE'] == torch.device('cuda:0'): \n","  model.cuda()\n","\n","#make a list of all the parameters that will get optimized (needed when using transfer learning)\n","optimize_params = [x for x in model.parameters() if x.requires_grad == True]\n","\n","#define the loss function and the optimizer\n","criterion = nn.CrossEntropyLoss()\n","if model_props['DEVICE'] == torch.device('cuda:0'): \n","  criterion.cuda()\n","optimizer = optim.SGD(optimize_params,\n","                      lr = model_props['LEARNING_RATE'], \n","                      momentum = model_props['MOMENTUM'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FQBylFulBNxA"},"source":["#training and evaluating the model\n","best_val_acc = -1.0\n","for epoch in range(model_props['N_EPOCHS']):\n","\n","  phase_loss = {'train': 0.0, 'val': 0.0}; phase_acc = {'train': 0.0, 'val': 0.0}\n","\n","  #Pytorch uses two phase system\n","  for phase in ['train', 'val']:\n","    if phase == 'train':\n","      model.train()  #model learns\n","    else:\n","      model.eval()   #model is just being evaluated\n","\n","    for inputs, labels in dataloaders_dict[phase]:\n","      #moving data to the GPU - if available\n","      inputs = inputs.to(model_props['DEVICE']) ; labels = labels.to(model_props['DEVICE'])\n","\n","      #gradients are being added - thus this line is needed\n","      optimizer.zero_grad()\n","      #gradient being calculate only if it is the training phase\n","      with torch.set_grad_enabled(phase == 'train'):\n","\n","        outputs = model(inputs) ; loss = criterion(outputs, labels)\n","\n","        _, preds = torch.max(outputs, 1)\n","\n","        #backprop only if it is the training phase\n","        if phase == 'train':\n","          loss.backward()\n","          optimizer.step()\n","\n","      #noting the progress\n","      phase_loss[phase] += loss.item() * inputs.size(0)\n","      phase_acc[phase] += torch.sum(preds == labels.data)\n","\n","    phase_loss[phase] = (phase_loss[phase] / len(dataloaders_dict[phase].dataset))\n","    phase_acc[phase] = (phase_acc[phase] / len(dataloaders_dict[phase].dataset)).item()\n","    \n","    #Saving the current model\n","    if phase == 'train' and epoch % model_props['SAVE_EVERY_N_EPOCHS']:\n","      torch.save(model.state_dict(), f\"{model_props['SAVE_PATH']}{model_props['TRAIN_NAME_SAVE']}\")\n","\n","    #keep the best model\n","    if phase == 'val' and best_val_acc < phase_acc[phase]:\n","      torch.save(model.state_dict(), f\"{model_props['SAVE_PATH']}{model_props['VAL_NAME_SAVE']}\")\n","      best_val_acc = phase_acc[phase]\n","\n","\n","  #append to the log file - to keep information of the progress through epochs\n","  try:\n","    with open(f\"{model_props['SAVE_PATH']}{model_props['LOG_NAME']}\") as f:\n","      data = json.load(f)\n","  except:\n","    data = {'epoch':0,\n","            'loss': [], \n","            'acc': []}\n","            \n","  data['best_val_acc'] = best_val_acc\n","  data['epoch'] += 1\n","  with open(f\"{model_props['SAVE_PATH']}{model_props['LOG_NAME']}\", 'w') as f:\n","    data['loss'].append(phase_loss)\n","    data['acc'].append(phase_acc)\n","    json.dump(data, f)\n","\n","  #print out the progress information\n","  print(f\"EPOCH: {data['epoch']}/{data['epoch'] + model_props['N_EPOCHS']} --------------------------------------------\")\n","  print(f\"\\tTrain phase: Loss: {phase_loss['train']} ; Acc: {phase_acc['train']}\")\n","  print(f\"\\tVal phase: Loss: {phase_loss['val']} ; Acc: {phase_acc['val']}\")\n"],"execution_count":null,"outputs":[]}]}