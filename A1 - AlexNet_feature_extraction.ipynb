{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"A1 - AlexNet_feature_extraction.ipynb","provenance":[{"file_id":"1I7MDL5LpqBZST-W-umNLNKnJUI5wt3Cv","timestamp":1620294954558}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPoSgweLKB6th26GH14RZxw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"22ad0d9598f94b72a875171c49e2ce3e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bfe5197dcac7460fb5dad533063e284e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d8ed8e41094048f498c385729d5eef1c","IPY_MODEL_55c5978de0834fcaae28327e350333bb"]}},"bfe5197dcac7460fb5dad533063e284e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d8ed8e41094048f498c385729d5eef1c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c30e0391c3d348c98de6f15d5ff6a6d5","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":244418560,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":244418560,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_06af49ba1b7d4dc9bacd22c9e269bae5"}},"55c5978de0834fcaae28327e350333bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_10e7b39d512b47cdae6a20a3202ff6f9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 233M/233M [00:07&lt;00:00, 32.3MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_85384a48b5724b58bf9b6dfa68045584"}},"c30e0391c3d348c98de6f15d5ff6a6d5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"06af49ba1b7d4dc9bacd22c9e269bae5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"10e7b39d512b47cdae6a20a3202ff6f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"85384a48b5724b58bf9b6dfa68045584":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"FCi3-TIuKWgY"},"source":["AlexNet trained using Transfer Learning (Feature Extraction)\n","------------\n","Training AlexNet model on Oxford-IIIT dataset using a variant of Transfer Learning.\n","\n","Parameters of all layers (except for the last fully connected layer) are frozen.\n","\n","Notes:\n","- Notebook expects that you have downloaded the dataset into your drive.\n","- Change the path to reflect your location."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7WViMhCr5AE_","executionInfo":{"status":"ok","timestamp":1620295276047,"user_tz":-120,"elapsed":66042,"user":{"displayName":"Milos Bojinovic","photoUrl":"","userId":"07446832655347204777"}},"outputId":"d79d93c2-c408-4c26-f74a-49ee95e4796a"},"source":["from google.colab import drive\n","\n","#mount the drive\n","MOUNTING_LOCATION = '/content/drive'\n","print('Mounting the drive...')\n","drive.mount(MOUNTING_LOCATION)\n","print(f\"Drive mounted at: {MOUNTING_LOCATION}\")\n","\n","#Unzip the train and validation datasets\n","print('Unzipping the train dataset...')\n","!unzip -q \"/content/drive/My Drive/DIPLOMSKI/oxford-iiit/train.zip\" -d '/content' \n","print('Finished unzipping the train dataset. Now unzipping the validation dataset...')\n","!unzip -q \"/content/drive/My Drive/DIPLOMSKI/oxford-iiit/val.zip\" -d '/content'\n","print('Finished unzipping the validation dataset.')\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounting the drive...\n","Mounted at /content/drive\n","Drive mounted at: /content/drive\n","Unzipping the train dataset...\n","Finished unzipping the train dataset. Now unzipping the validation dataset...\n","Finished unzipping the validation dataset.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S9xHpyb98Ixw","executionInfo":{"status":"ok","timestamp":1620295374734,"user_tz":-120,"elapsed":1169,"user":{"displayName":"Milos Bojinovic","photoUrl":"","userId":"07446832655347204777"}}},"source":["import torch\n","from PIL import Image\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import time\n","import os\n","import json\n","import copy\n","\n","dataset_props = {'DATA_DIR': '/content/',\n","                 'N_CLASSES': 37\n","                 }\n","\n","MODEL_NAME = 'A1'\n","model_props = {'MODEL_NAME': MODEL_NAME,\n","               'PRETRAINED': True,\n","               'SAVE_PATH': f'/content/drive/My Drive/DIPLOMSKI/{MODEL_NAME}/',\n","               'SAVE_EVERY_N_EPOCHS': 5,\n","               'TRAIN_NAME_SAVE': f'{MODEL_NAME}_train_model.pt',\n","               'VAL_NAME_SAVE': f'{MODEL_NAME}_val_model.pt',\n","               'LOG_NAME': f'{MODEL_NAME}_log.json',\n","               'INPUT_SIZE': 224,\n","               'N_EPOCHS': 1000,\n","               'BATCH_SIZE': 64,\n","               'LEARNING_RATE': 1e-4,\n","               'MOMENTUM': 0.9,\n","               'DEVICE': torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")}\n","\n","#transforms to be applied to images before them being fed to the model\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(model_props['INPUT_SIZE']),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(model_props['INPUT_SIZE']),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}\n","#building the dataset loaders\n","image_datasets = {x: datasets.ImageFolder(os.path.join(dataset_props['DATA_DIR'], x), \n","                                          data_transforms[x]) for x in ['train', 'val']}\n","dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x],\n","                                                   batch_size=model_props['BATCH_SIZE'],\n","                                                   shuffle=True) for x in ['train', 'val']}\n","\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":100,"referenced_widgets":["22ad0d9598f94b72a875171c49e2ce3e","bfe5197dcac7460fb5dad533063e284e","d8ed8e41094048f498c385729d5eef1c","55c5978de0834fcaae28327e350333bb","c30e0391c3d348c98de6f15d5ff6a6d5","06af49ba1b7d4dc9bacd22c9e269bae5","10e7b39d512b47cdae6a20a3202ff6f9","85384a48b5724b58bf9b6dfa68045584"]},"id":"ePBpQcUt_e7v","executionInfo":{"status":"ok","timestamp":1620295381994,"user_tz":-120,"elapsed":3928,"user":{"displayName":"Milos Bojinovic","photoUrl":"","userId":"07446832655347204777"}},"outputId":"559dfaa1-d839-4ae1-8414-5afbc658d6ee"},"source":["\n","#Build the Model\n","model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=model_props['PRETRAINED'])\n","\n","#freeze all parameters\n","for param in model.parameters():\n","  param.requires_grad = False\n","\n","#Replace the last fullyconnected layer - to match the number of classes in this dataset\n","num_ftrs = model.classifier[6].in_features\n","model.classifier[6] = nn.Linear(num_ftrs, dataset_props['N_CLASSES'])\n","\n","#put the model inside GPU memory if GPU is available\n","if model_props['DEVICE'] == torch.device('cuda:0'): \n","  model.cuda()\n","\n","#make a list of all the parameters that will get optimized (needed when using transfer learning)\n","optimize_params = [x for x in model.parameters() if x.requires_grad == True]\n","\n","#define the loss function and the optimizer\n","criterion = nn.CrossEntropyLoss()\n","if model_props['DEVICE'] == torch.device('cuda:0'): \n","  criterion.cuda()\n","optimizer = optim.SGD(optimize_params,\n","                      lr = model_props['LEARNING_RATE'], \n","                      momentum = model_props['MOMENTUM'])\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n","Downloading: \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-4df8aa71.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"22ad0d9598f94b72a875171c49e2ce3e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=244418560.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"FQBylFulBNxA","executionInfo":{"status":"error","timestamp":1620296693869,"user_tz":-120,"elapsed":1307526,"user":{"displayName":"Milos Bojinovic","photoUrl":"","userId":"07446832655347204777"}},"outputId":"dcbcfe51-ebb4-4658-f78d-2c14e83a1751"},"source":["#training and evaluating the model\n","best_val_acc = -1.0\n","for epoch in range(model_props['N_EPOCHS']):\n","\n","  phase_loss = {'train': 0.0, 'val': 0.0}; phase_acc = {'train': 0.0, 'val': 0.0}\n","\n","  #Pytorch uses two phase system\n","  for phase in ['train', 'val']:\n","    if phase == 'train':\n","      model.train()  #model learns\n","    else:\n","      model.eval()   #model is just being evaluated\n","\n","    for inputs, labels in dataloaders_dict[phase]:\n","      #moving data to the GPU - if available\n","      inputs = inputs.to(model_props['DEVICE']) ; labels = labels.to(model_props['DEVICE'])\n","\n","      #gradients are being added - thus this line is needed\n","      optimizer.zero_grad()\n","      #gradient being calculate only if it is the training phase\n","      with torch.set_grad_enabled(phase == 'train'):\n","\n","        outputs = model(inputs) ; loss = criterion(outputs, labels)\n","\n","        _, preds = torch.max(outputs, 1)\n","\n","        #backprop only if it is the training phase\n","        if phase == 'train':\n","          loss.backward()\n","          optimizer.step()\n","\n","      #noting the progress\n","      phase_loss[phase] += loss.item() * inputs.size(0)\n","      phase_acc[phase] += torch.sum(preds == labels.data)\n","\n","    phase_loss[phase] = (phase_loss[phase] / len(dataloaders_dict[phase].dataset))\n","    phase_acc[phase] = (phase_acc[phase] / len(dataloaders_dict[phase].dataset)).item()\n","    \n","    #Saving the current model\n","    if phase == 'train' and epoch % model_props['SAVE_EVERY_N_EPOCHS']:\n","      torch.save(model.state_dict(), f\"{model_props['SAVE_PATH']}{model_props['TRAIN_NAME_SAVE']}\")\n","\n","    #keep the best model\n","    if phase == 'val' and best_val_acc < phase_acc[phase]:\n","      torch.save(model.state_dict(), f\"{model_props['SAVE_PATH']}{model_props['VAL_NAME_SAVE']}\")\n","      best_val_acc = phase_acc[phase]\n","\n","\n","  #append to the log file - to keep information of the progress through epochs\n","  try:\n","    with open(f\"{model_props['SAVE_PATH']}{model_props['LOG_NAME']}\") as f:\n","      data = json.load(f)\n","  except:\n","    data = {'epoch':0,\n","            'loss': [], \n","            'acc': []}\n","            \n","  data['best_val_acc'] = best_val_acc\n","  data['epoch'] += 1\n","  with open(f\"{model_props['SAVE_PATH']}{model_props['LOG_NAME']}\", 'w') as f:\n","    data['loss'].append(phase_loss)\n","    data['acc'].append(phase_acc)\n","    json.dump(data, f)\n","\n","  #print out the progress information\n","  print(f\"EPOCH: {data['epoch']}/{data['epoch'] + model_props['N_EPOCHS']} --------------------------------------------\")\n","  print(f\"\\tTrain phase: Loss: {phase_loss['train']} ; Acc: {phase_acc['train']}\")\n","  print(f\"\\tVal phase: Loss: {phase_loss['val']} ; Acc: {phase_acc['val']}\")\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["EPOCH: 257/1257 --------------------------------------------\n","\tTrain phase: Loss: 3.349905349717674 ; Acc: 0.12906137108802795\n","\tVal phase: Loss: 2.4605289700068407 ; Acc: 0.4035206437110901\n","EPOCH: 258/1258 --------------------------------------------\n","\tTrain phase: Loss: 2.3490520513445032 ; Acc: 0.40500903129577637\n","\tVal phase: Loss: 1.6845996754348642 ; Acc: 0.6269465088844299\n","EPOCH: 259/1259 --------------------------------------------\n","\tTrain phase: Loss: 1.9125495068863412 ; Acc: 0.5282039642333984\n","\tVal phase: Loss: 1.3381380251152901 ; Acc: 0.7034529447555542\n","EPOCH: 260/1260 --------------------------------------------\n","\tTrain phase: Loss: 1.6687976100384543 ; Acc: 0.5810018181800842\n","\tVal phase: Loss: 1.1537295880960143 ; Acc: 0.7278266549110413\n","EPOCH: 261/1261 --------------------------------------------\n","\tTrain phase: Loss: 1.492348938642426 ; Acc: 0.6198104619979858\n","\tVal phase: Loss: 1.0365130458236467 ; Acc: 0.7447528839111328\n","EPOCH: 262/1262 --------------------------------------------\n","\tTrain phase: Loss: 1.3983025985504316 ; Acc: 0.637635350227356\n","\tVal phase: Loss: 0.9536497513829815 ; Acc: 0.7542315125465393\n","EPOCH: 263/1263 --------------------------------------------\n","\tTrain phase: Loss: 1.3279894048126166 ; Acc: 0.6552346348762512\n","\tVal phase: Loss: 0.8955132187260271 ; Acc: 0.7643872499465942\n","EPOCH: 264/1264 --------------------------------------------\n","\tTrain phase: Loss: 1.2842739768837332 ; Acc: 0.6511732935905457\n","\tVal phase: Loss: 0.8491957297418758 ; Acc: 0.7725118398666382\n","EPOCH: 265/1265 --------------------------------------------\n","\tTrain phase: Loss: 1.2209714651107788 ; Acc: 0.6789259910583496\n","\tVal phase: Loss: 0.8124897693893598 ; Acc: 0.7833446264266968\n","EPOCH: 266/1266 --------------------------------------------\n","\tTrain phase: Loss: 1.1739288663175562 ; Acc: 0.6886281371116638\n","\tVal phase: Loss: 0.7847007085281041 ; Acc: 0.7819905281066895\n","EPOCH: 267/1267 --------------------------------------------\n","\tTrain phase: Loss: 1.1751390015605554 ; Acc: 0.6771209239959717\n","\tVal phase: Loss: 0.7606348225452162 ; Acc: 0.7867298126220703\n","EPOCH: 268/1268 --------------------------------------------\n","\tTrain phase: Loss: 1.1550424357183573 ; Acc: 0.6807310581207275\n","\tVal phase: Loss: 0.7398277437194587 ; Acc: 0.7948544025421143\n","EPOCH: 269/1269 --------------------------------------------\n","\tTrain phase: Loss: 1.0995831560572131 ; Acc: 0.6913357377052307\n","\tVal phase: Loss: 0.7216841639016396 ; Acc: 0.7928232550621033\n","EPOCH: 270/1270 --------------------------------------------\n","\tTrain phase: Loss: 1.05244132370725 ; Acc: 0.7093862891197205\n","\tVal phase: Loss: 0.7049754056562503 ; Acc: 0.7995937466621399\n","EPOCH: 271/1271 --------------------------------------------\n","\tTrain phase: Loss: 1.0374894632759508 ; Acc: 0.7136732935905457\n","\tVal phase: Loss: 0.6896381772847779 ; Acc: 0.8043330907821655\n","EPOCH: 272/1272 --------------------------------------------\n","\tTrain phase: Loss: 1.053020809316463 ; Acc: 0.7084837555885315\n","\tVal phase: Loss: 0.678569854718249 ; Acc: 0.8070412874221802\n","EPOCH: 273/1273 --------------------------------------------\n","\tTrain phase: Loss: 1.0183146978973912 ; Acc: 0.7195397019386292\n","\tVal phase: Loss: 0.6695709451276383 ; Acc: 0.8077183365821838\n","EPOCH: 274/1274 --------------------------------------------\n","\tTrain phase: Loss: 1.0168353910910954 ; Acc: 0.7078068256378174\n","\tVal phase: Loss: 0.6582797994142299 ; Acc: 0.8070412874221802\n","EPOCH: 275/1275 --------------------------------------------\n","\tTrain phase: Loss: 1.0013731812742213 ; Acc: 0.7199909687042236\n","\tVal phase: Loss: 0.6492933038125267 ; Acc: 0.8138117790222168\n","EPOCH: 276/1276 --------------------------------------------\n","\tTrain phase: Loss: 0.9809443602062735 ; Acc: 0.7247292399406433\n","\tVal phase: Loss: 0.640686487033711 ; Acc: 0.8117806315422058\n","EPOCH: 277/1277 --------------------------------------------\n","\tTrain phase: Loss: 1.011342580568059 ; Acc: 0.7096118927001953\n","\tVal phase: Loss: 0.6345853246218786 ; Acc: 0.8111035823822021\n","EPOCH: 278/1278 --------------------------------------------\n","\tTrain phase: Loss: 0.9576487709038525 ; Acc: 0.7292418479919434\n","\tVal phase: Loss: 0.6282724119475668 ; Acc: 0.8124576807022095\n","EPOCH: 279/1279 --------------------------------------------\n","\tTrain phase: Loss: 0.9402763456213775 ; Acc: 0.7351083159446716\n","\tVal phase: Loss: 0.6223473971004654 ; Acc: 0.8151658773422241\n","EPOCH: 280/1280 --------------------------------------------\n","\tTrain phase: Loss: 0.9290365815593017 ; Acc: 0.7202165722846985\n","\tVal phase: Loss: 0.6165440739331087 ; Acc: 0.8138117790222168\n","EPOCH: 281/1281 --------------------------------------------\n","\tTrain phase: Loss: 0.92975049866666 ; Acc: 0.7335288524627686\n","\tVal phase: Loss: 0.6103226670802406 ; Acc: 0.8178740739822388\n","EPOCH: 282/1282 --------------------------------------------\n","\tTrain phase: Loss: 0.9399512309029645 ; Acc: 0.7197653651237488\n","\tVal phase: Loss: 0.6043646112841049 ; Acc: 0.8165199756622314\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-a4104da8692b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#model is just being evaluated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m       \u001b[0;31m#moving data to the GPU - if available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_props\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DEVICE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m;\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_props\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DEVICE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \"\"\"\n\u001b[1;32m    177\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    899\u001b[0m         \"\"\"\n\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}